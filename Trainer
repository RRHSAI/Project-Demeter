import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import pandas as pd

# Custom Dataset for Pre-Tokenized Chatbot Data
class ChatDataset(Dataset):
    def __init__(self, data_path):
        self.data = pd.read_csv(data_path)  # Assume CSV has 'input_ids' and 'response_ids' columns

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        # Directly use pre-tokenized input and response IDs
        input_ids = torch.tensor(eval(self.data.iloc[idx]['question_words']), dtype=torch.long)
        response_ids = torch.tensor(eval(self.data.iloc[idx]['answer_words']), dtype=torch.long)
        
        return question_words, answer_words

# Define the DeepSeek model architecture
class DeepSeekModel(nn.Module):
    def __init__(self, num_layers=61, hidden_size=768, vocab_size=10000):
        super(DeepSeekModel, self).__init__()
        self.embedding = nn.Embedding(vocab_size, hidden_size)
        self.encoder_layers = nn.ModuleList(
            [nn.TransformerEncoderLayer(d_model=hidden_size, nhead=8) for _ in range(num_layers)]
        )
        self.decoder = nn.Linear(hidden_size, vocab_size)  # Decoder layer for generating responses

    def forward(self, input_ids):
        x = self.embedding(input_ids)  # Embedding layer
        x = x.transpose(0, 1)          # Match transformer input shape
        for layer in self.encoder_layers:
            x = layer(x)
        x = x.transpose(0, 1)          # Back to original shape
        logits = self.decoder(x)       # Output vocabulary logits
        return logits

# Prepare Dataset and DataLoader
data_path = "tokenized_data.csv"  # Path to your dataset containing pre-tokenized data
dataset = ChatDataset(data_path)
dataloader = DataLoader(dataset, batch_size=8, shuffle=True)

# Initialize the model
vocab_size = 10000  # Adjust according to your model configuration
model = DeepSeekModel(num_layers=61, hidden_size=768, vocab_size=vocab_size)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# Define loss function and optimizer
criterion = nn.CrossEntropyLoss(ignore_index=0)  # Adjust ignore index according to your padding token ID
optimizer = optim.AdamW(model.parameters(), lr=5e-5)

# Training loop
num_epochs = 3
model.train()

for epoch in range(num_epochs):
    epoch_loss = 0
    for input_ids, response_ids in dataloader:
        input_ids, response_ids = input_ids.to(device), response_ids.to(device)

        # Forward pass
        logits = model(input_ids)

        # Reshape logits and labels for loss calculation
        logits = logits.view(-1, vocab_size)
        response_ids = response_ids.view(-1)

        # Calculate loss
        loss = criterion(logits, response_ids)

        # Backward pass and optimization
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        epoch_loss += loss.item()

    avg_loss = epoch_loss / len(dataloader)
    print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}")

# Save the fine-tuned model
torch.save(model.state_dict(), "deepseek_finetuned_weights.pth")
print("Fine-tuned DeepSeek model saved!")
