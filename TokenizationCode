import nltk
import pandas as pd
from nltk.tokenize import word_tokenize, sent_tokenize

# Downloading necessary NLTK resources
nltk.download('punkt_tab')

# Input CSV + Output CSV
input_file_name = 'example.csv'
output_file_name = 'tokenized_output.csv'

# Load the CSV file 
df = pd.read_csv(input_file_name)

# Create an empty DataFrame to store the tokenized output
tokenized_df = pd.DataFrame(columns=['Column', 'Original_Text', 'Tokenized_Sentences', 'Tokenized_Words'])

# Go through each column
for column in df.columns:
    for text in df[column].dropna():  
        text = str(text)  # Convert value to string (if it's not already text)
        
        # Tokenizing sentences
        sentences = sent_tokenize(text)
        
        # Tokenizing words
        words = word_tokenize(text)
        
        # Add the tokenized data to the new DataFrame
        tokenized_df = pd.concat([
            tokenized_df,
            pd.DataFrame({
                'Column': [column],
                'Original_Text': [text],
                'Tokenized_Sentences': [sentences],
                'Tokenized_Words': [words]
            })
        ], ignore_index=True)

# Save the tokenized DataFrame into a CSV file
tokenized_df.to_csv(output_file_name, index=False)

