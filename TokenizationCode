import nltk
import pandas as pd
from nltk.tokenize import word_tokenize, sent_tokenize

# Downloading NLTK resources
nltk.download('punkt')

# Use file here
file_name = 'example.csv'

# Load CSV
df = pd.read_csv(file_name)

for column in df.columns:
    for text in df[column].dropna():  
        # Convert value to string (if it's not already text)
        text = str(text)
        
        # Tokenizing sentences
        sentences = sent_tokenize(text)
        print("Tokenized Sentences:")
        print(sentences)
        
        # Tokenizing words
        words = word_tokenize(text)
        print("\nTokenized Words:")
        print(words)
        print("-" * 50)
